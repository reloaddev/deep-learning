{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"dair-ai/emotion\", \"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    0.335125\n",
      "0    0.291625\n",
      "3    0.134937\n",
      "4    0.121063\n",
      "2    0.081500\n",
      "5    0.035750\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      "1    0.3475\n",
      "0    0.2905\n",
      "3    0.1375\n",
      "4    0.1120\n",
      "2    0.0795\n",
      "5    0.0330\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      "1    0.3520\n",
      "0    0.2750\n",
      "3    0.1375\n",
      "4    0.1060\n",
      "2    0.0890\n",
      "5    0.0405\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### Task 1.1 ###\n",
    "# Extract class labels into list of integers\n",
    "\n",
    "train_df = ds[\"train\"].to_pandas()\n",
    "test_df = ds[\"test\"].to_pandas() \n",
    "validation_df = ds[\"validation\"].to_pandas()\n",
    "\n",
    "train_dist = train_df['label'].value_counts(normalize=True)\n",
    "test_dist = test_df['label'].value_counts(normalize=True)\n",
    "validation_dist = validation_df['label'].value_counts(normalize=True)\n",
    "\n",
    "print(train_dist)\n",
    "print(test_dist)\n",
    "print(validation_dist)\n",
    "\n",
    "# The class distribution is not balanced, but the balance is the\n",
    "# same across all three splitsacc=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chance Levels\n",
      "Train:  0.2381384765625\n",
      "Test:  0.24400599999999997\n",
      "Validation:  0.23923250000000004\n"
     ]
    }
   ],
   "source": [
    "# What is the chance accuracy level?\n",
    "\n",
    "chance_level_train = (train_dist** 2).sum()\n",
    "chance_level_test = (test_dist ** 2).sum()\n",
    "chance_level_val = (validation_dist ** 2).sum()\n",
    "\n",
    "print(\"Chance Levels\")\n",
    "print(\"Train: \", chance_level_train)\n",
    "print(\"Test: \", chance_level_test)\n",
    "print(\"Validation: \", chance_level_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier only predicting most common class:  0.08936666666666666\n"
     ]
    }
   ],
   "source": [
    "# What would be the accuracy of a classifier\n",
    "# that only predicts the most common class seen in training?\n",
    "\n",
    "print(\"Accuracy of classifier only predicting most common class: \", 5362/60000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Text Length - Range              : 293\n",
      "[Train] Text Length - Mean               : 96.8458125\n",
      "[Train] Text Length - Std                : 55.904952812332766\n",
      "[Test] Text Length - Range              : 282\n",
      "[Test] Text Length - Mean               : 96.5865\n",
      "[Test] Text Length - Std                : 55.71599100417033\n",
      "[Validation] Text Length - Range              : 284\n",
      "[Validation] Text Length - Mean               : 95.3475\n",
      "[Validation] Text Length - Std                : 54.82375913810559\n",
      "                                                text  label  \\\n",
      "0                            i didnt feel humiliated      0   \n",
      "1  i can go from feeling so hopeless to so damned...      0   \n",
      "2   im grabbing a minute to post i feel greedy wrong      3   \n",
      "3  i am ever feeling nostalgic about the fireplac...      2   \n",
      "4                               i am feeling grouchy      3   \n",
      "\n",
      "                                              tokens  \n",
      "0                       [i, didnt, feel, humiliated]  \n",
      "1  [i, can, go, from, feeling, so, hopeless, to, ...  \n",
      "2  [im, grabbing, a, minute, to, post, i, feel, g...  \n",
      "3  [i, am, ever, feeling, nostalgic, about, the, ...  \n",
      "4                          [i, am, feeling, grouchy]  \n"
     ]
    }
   ],
   "source": [
    "### Task 1.2 ###\n",
    "# Analyze the distribution of text lengths by providing its range, mean and standard deviation.\n",
    "\n",
    "splits = [\n",
    "    {\"label\": \"Train\", \"df\": train_df},\n",
    "    {\"label\": \"Test\", \"df\": test_df},\n",
    "    {\"label\": \"Validation\", \"df\": validation_df}\n",
    "]\n",
    "for split in splits:    \n",
    "    text_lengths = split[\"df\"][\"text\"].map(lambda x: len(x))\n",
    "    text_lengths_range = text_lengths.max() - text_lengths.min()\n",
    "    print(f\"[{split['label']}] Text Length - Range              :\", text_lengths_range)\n",
    "    text_lengths_mean = text_lengths.mean()\n",
    "    print(f\"[{split['label']}] Text Length - Mean               :\", text_lengths_mean)\n",
    "    text_lengths_std = text_lengths.std()\n",
    "    print(f\"[{split['label']}] Text Length - Std                :\", text_lengths_std)\n",
    "\n",
    "\n",
    "# Extract the texts for all splits and split each text into tokens.\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "train_df[\"tokens\"] = train_df[\"text\"].apply(lambda x: tokenizer(x))\n",
    "test_df[\"tokens\"] = test_df[\"text\"].apply(lambda x: tokenizer(x))\n",
    "validation_df[\"tokens\"] = validation_df[\"text\"].apply(lambda x: tokenizer(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 1.3 ###\n",
    "# Build a vocabulary (map string to integer) based on train split\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "counter = Counter()\n",
    "for sample in train_df[\"tokens\"]:\n",
    "    counter.update(sample)\n",
    "vocabulary = {\n",
    "    '<UNK>': 0,\n",
    "    '<PAD>': 1,\n",
    "    **{word: idx + 2 for idx, (word, count) in enumerate(counter.most_common(1000))}   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 1.4 ###\n",
    "# Encode all texts with the defined vocabulary\n",
    "# value 0 resembles <UNK> (unknown token)\n",
    "# value 1 resemples <PAD> (padding token)\n",
    "\n",
    "# Sequences shorter than max_length, will be filled\n",
    "# up with <PAD> until they match max_length\n",
    "def pad_sequence(sequence, max_length=100, pad_value=1):\n",
    "    if len(sequence) > max_length:\n",
    "        return sequence[:max_length]\n",
    "    else:\n",
    "        return sequence + [pad_value] * (max_length - len(sequence))\n",
    "\n",
    "# Encode and pad all texts with the defined vocabulary\n",
    "train_sequences = [pad_sequence([vocabulary.get(token, 0) for token in sample]) for sample in train_df[\"tokens\"]]\n",
    "test_sequences = [pad_sequence([vocabulary.get(token, 0) for token in sample]) for sample in test_df[\"tokens\"]]\n",
    "validation_sequences = [pad_sequence([vocabulary.get(token, 0) for token in sample]) for sample in validation_df[\"tokens\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 1.5a ###\n",
    "# Convert lists into tensors\n",
    "def vectorize_sequences(sequences, samples, vocabulary):\n",
    "    one_hot_results = torch.zeros(len(samples), len(vocabulary) + 1)\n",
    "    for idx, sequence in enumerate(sequences):\n",
    "        one_hot_results[idx, sequence] = 1\n",
    "    return one_hot_results\n",
    "\n",
    "train_data = vectorize_sequences(train_sequences, train_df[\"text\"],vocabulary)\n",
    "test_data = vectorize_sequences(test_sequences, test_df[\"text\"],vocabulary)\n",
    "validation_data = vectorize_sequences(validation_sequences, validation_df[\"text\"],vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 1.5b ###\n",
    "# Load the data\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Assuming you have labels in your DataFrame\n",
    "train_labels = train_df[\"label\"].tolist()\n",
    "test_labels = test_df[\"label\"].tolist()\n",
    "validation_labels = validation_df[\"label\"].tolist()\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = TextDataset(train_sequences, train_labels)\n",
    "test_dataset = TextDataset(test_sequences, test_labels)\n",
    "validation_dataset = TextDataset(validation_sequences, validation_labels)\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2 ###\n",
    "# Design a model that is suitable for the task. Network 1 --> RNN\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_size=200, num_layers=3, num_classes=6):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)  # PAD token index is 1\n",
    "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: (batch_size, max_length, embedding_dim)\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size)\n",
    "        # Pass through RNN\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        # Extract the last hidden state\n",
    "        hidden_state_outputs = out[:, -1, :]\n",
    "        # Pass through fully connected layer\n",
    "        result = self.fc(hidden_state_outputs)\n",
    "        return result\n",
    "   \n",
    "vocab_size = len(vocabulary) \n",
    "model = RNNModel(vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.606830 [    0/16000]\n",
      "loss: 1.577473 [ 3200/16000]\n",
      "loss: 1.488990 [ 6400/16000]\n",
      "loss: 1.634478 [ 9600/16000]\n",
      "loss: 1.589266 [12800/16000]\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 1.560201 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.550612 [    0/16000]\n",
      "loss: 1.585140 [ 3200/16000]\n",
      "loss: 1.393740 [ 6400/16000]\n",
      "loss: 1.617930 [ 9600/16000]\n",
      "loss: 1.660636 [12800/16000]\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 1.560559 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.566757 [    0/16000]\n",
      "loss: 1.429015 [ 3200/16000]\n",
      "loss: 1.489953 [ 6400/16000]\n",
      "loss: 1.439544 [ 9600/16000]\n",
      "loss: 1.604355 [12800/16000]\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 1.560739 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.660555 [    0/16000]\n",
      "loss: 1.662458 [ 3200/16000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     test_loop(test_loader, model, loss_fn)\n\u001b[1;32m     91\u001b[0m validate(validation_loader, model, loss_fn)\n",
      "Cell \u001b[0;32mIn[86], line 25\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/workspace/study/deep-learning/project-work/project-2/.venv/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/study/deep-learning/project-work/project-2/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 0.00001\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    train_losses.append(running_loss / len(dataloader))\n",
    "    train_accuracies.append(correct / total) \n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            \n",
    "    test_losses.append(test_loss / num_batches)  # Average test loss\n",
    "    test_accuracies.append(correct / total)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "def validate(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    validation_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            validation_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    validation_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {validation_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    test_loop(test_loader, model, loss_fn)\n",
    "validate(validation_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
